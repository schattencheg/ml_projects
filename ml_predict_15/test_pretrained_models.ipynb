{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Pretrained Models\n",
    "\n",
    "This notebook loads pretrained models from a selected folder and tests them against a dataset.\n",
    "\n",
    "## Steps:\n",
    "1. Select a training session folder\n",
    "2. Load models and scaler\n",
    "3. Load test dataset\n",
    "4. Evaluate models\n",
    "5. Display results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.data_preparation import prepare_data\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List Available Training Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all training sessions\n",
    "models_base = 'models'\n",
    "sessions = sorted([d for d in os.listdir(models_base) \n",
    "                   if os.path.isdir(os.path.join(models_base, d))],\n",
    "                  reverse=True)\n",
    "\n",
    "print(f\"Found {len(sessions)} training session(s):\\n\")\n",
    "for i, session in enumerate(sessions[:10], 1):  # Show last 10 sessions\n",
    "    session_path = os.path.join(models_base, session)\n",
    "    \n",
    "    # Try to read config file\n",
    "    config_path = os.path.join(session_path, 'training_config.txt')\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            # Extract best model info\n",
    "            best_model = \"N/A\"\n",
    "            best_acc = \"N/A\"\n",
    "            for line in lines:\n",
    "                if \"Best Model:\" in line:\n",
    "                    best_model = line.split(\":\")[1].strip()\n",
    "                if \"Best Accuracy:\" in line:\n",
    "                    best_acc = line.split(\":\")[1].strip()\n",
    "        print(f\"{i}. {session}\")\n",
    "        print(f\"   Best Model: {best_model}, Accuracy: {best_acc}\\n\")\n",
    "    else:\n",
    "        print(f\"{i}. {session}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Training Session\n",
    "\n",
    "Choose which training session to load (default: latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select session (change this to select a different session)\n",
    "# Option 1: Use latest session\n",
    "selected_session = sessions[0]\n",
    "\n",
    "# Option 2: Manually specify session\n",
    "# selected_session = '2024-01-15_14-30-45'\n",
    "\n",
    "session_path = os.path.join(models_base, selected_session)\n",
    "\n",
    "print(f\"Selected session: {selected_session}\")\n",
    "print(f\"Session path: {session_path}\")\n",
    "\n",
    "# Display training configuration\n",
    "config_path = os.path.join(session_path, 'training_config.txt')\n",
    "if os.path.exists(config_path):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING CONFIGURATION\")\n",
    "    print(\"=\"*80)\n",
    "    with open(config_path, 'r') as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Models and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scaler\n",
    "scaler_path = os.path.join(session_path, 'scaler.joblib')\n",
    "scaler = joblib.load(scaler_path)\n",
    "print(f\"✓ Scaler loaded from: {scaler_path}\")\n",
    "\n",
    "# Load all models\n",
    "models = {}\n",
    "for file in os.listdir(session_path):\n",
    "    if file.endswith('.joblib') and file != 'scaler.joblib' and not file.endswith('_best.joblib'):\n",
    "        model_name = file.replace('.joblib', '')\n",
    "        model_path = os.path.join(session_path, file)\n",
    "        models[model_name] = joblib.load(model_path)\n",
    "        print(f\"✓ Loaded: {model_name}\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Test Dataset\n",
    "\n",
    "Specify your test dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "# Change this to your test dataset path\n",
    "test_data_path = 'data/btc_2023.csv'\n",
    "\n",
    "print(f\"Loading test data from: {test_data_path}\")\n",
    "df_test = pd.read_csv(test_data_path)\n",
    "\n",
    "print(f\"✓ Test data loaded\")\n",
    "print(f\"  Shape: {df_test.shape}\")\n",
    "print(f\"  Date range: {df_test['time'].min()} to {df_test['time'].max()}\")\n",
    "\n",
    "# Display first few rows\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "# Use same parameters as training (adjust if needed)\n",
    "target_bars = 45\n",
    "target_pct = 3.0\n",
    "\n",
    "print(f\"Preparing test data with target_bars={target_bars}, target_pct={target_pct}%\")\n",
    "X_test, y_test = prepare_data(df_test, target_bars=target_bars, target_pct=target_pct)\n",
    "\n",
    "print(f\"✓ Test data prepared\")\n",
    "print(f\"  Features shape: {X_test.shape}\")\n",
    "print(f\"  Target shape: {y_test.shape}\")\n",
    "\n",
    "# Display class distribution\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y_test.value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scale Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"✓ Test data scaled\")\n",
    "print(f\"  Scaled features shape: {X_test_scaled.shape}\")\n",
    "print(f\"  Min value: {X_test_scaled.min():.4f}\")\n",
    "print(f\"  Max value: {X_test_scaled.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATING MODELS ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating: {model_name}...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Get prediction probabilities if available\n",
    "    try:\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    except:\n",
    "        roc_auc = np.nan\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1_Score': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'ROC_AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    if not np.isnan(roc_auc):\n",
    "        print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1_Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_f1 = results_df.iloc[0]['F1_Score']\n",
    "print(f\"\\n✓ Best Model: {best_model_name} (F1 Score: {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Model Performance on Test Set', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['Accuracy', 'F1_Score', 'Precision', 'Recall', 'ROC_AUC']\n",
    "titles = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC AUC']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Sort by metric\n",
    "    sorted_df = results_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.barh(sorted_df['Model'], sorted_df[metric])\n",
    "    \n",
    "    # Color best model differently\n",
    "    best_idx = sorted_df[metric].idxmax()\n",
    "    bars[list(sorted_df.index).index(best_idx)].set_color('green')\n",
    "    \n",
    "    ax.set_xlabel(title, fontsize=12)\n",
    "    ax.set_title(f'{title} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (idx, row) in enumerate(sorted_df.iterrows()):\n",
    "        value = row[metric]\n",
    "        if not np.isnan(value):\n",
    "            ax.text(value, i, f' {value:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detailed Analysis of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"DETAILED ANALYSIS: {best_model_name.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['No Increase', 'Increase']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Increase', 'Increase'],\n",
    "            yticklabels=['No Increase', 'Increase'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives:  {tn:,}\")\n",
    "print(f\"  False Positives: {fp:,}\")\n",
    "print(f\"  False Negatives: {fn:,}\")\n",
    "print(f\"  True Positives:  {tp:,}\")\n",
    "\n",
    "print(f\"\\nAdditional Metrics:\")\n",
    "print(f\"  Specificity: {tn / (tn + fp):.4f}\")\n",
    "print(f\"  Sensitivity (Recall): {tp / (tp + fn):.4f}\")\n",
    "print(f\"  False Positive Rate: {fp / (fp + tn):.4f}\")\n",
    "print(f\"  False Negative Rate: {fn / (fn + tp):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare Training vs Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training results\n",
    "training_csv_path = os.path.join(session_path, 'training_results_summary.csv')\n",
    "\n",
    "if os.path.exists(training_csv_path):\n",
    "    training_results = pd.read_csv(training_csv_path)\n",
    "    \n",
    "    # Filter out summary rows\n",
    "    training_results = training_results[~training_results['Model'].str.contains('SUMMARY|Best Model|Total|Average', na=False)]\n",
    "    \n",
    "    # Merge with test results\n",
    "    comparison = training_results[['Model', 'Accuracy', 'F1_Score']].merge(\n",
    "        results_df[['Model', 'Accuracy', 'F1_Score']],\n",
    "        on='Model',\n",
    "        suffixes=('_Train', '_Test')\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TRAINING vs TEST PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(comparison.to_string(index=False))\n",
    "    \n",
    "    # Calculate performance drop\n",
    "    comparison['Accuracy_Drop'] = comparison['Accuracy_Train'] - comparison['Accuracy_Test']\n",
    "    comparison['F1_Drop'] = comparison['F1_Score_Train'] - comparison['F1_Score_Test']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERFORMANCE DROP (Training - Test)\")\n",
    "    print(\"=\"*80)\n",
    "    print(comparison[['Model', 'Accuracy_Drop', 'F1_Drop']].to_string(index=False))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    x = np.arange(len(comparison))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0].bar(x - width/2, comparison['Accuracy_Train'], width, label='Training', alpha=0.8)\n",
    "    axes[0].bar(x + width/2, comparison['Accuracy_Test'], width, label='Test', alpha=0.8)\n",
    "    axes[0].set_xlabel('Model', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Accuracy: Training vs Test', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(comparison['Model'], rotation=45, ha='right')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # F1 Score comparison\n",
    "    axes[1].bar(x - width/2, comparison['F1_Score_Train'], width, label='Training', alpha=0.8)\n",
    "    axes[1].bar(x + width/2, comparison['F1_Score_Test'], width, label='Test', alpha=0.8)\n",
    "    axes[1].set_xlabel('Model', fontsize=12)\n",
    "    axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "    axes[1].set_title('F1 Score: Training vs Test', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(comparison['Model'], rotation=45, ha='right')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Comparison complete\")\n",
    "else:\n",
    "    print(\"Training results CSV not found. Skipping comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test results to session directory\n",
    "test_results_path = os.path.join(session_path, 'test_results.csv')\n",
    "results_df.to_csv(test_results_path, index=False)\n",
    "\n",
    "print(f\"✓ Test results saved to: {test_results_path}\")\n",
    "\n",
    "# Save detailed report\n",
    "report_path = os.path.join(session_path, 'test_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"TEST RESULTS REPORT\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(f\"Training Session: {selected_session}\\n\")\n",
    "    f.write(f\"Test Dataset: {test_data_path}\\n\")\n",
    "    f.write(f\"Test Set Size: {len(y_test)}\\n\")\n",
    "    f.write(f\"Target Bars: {target_bars}\\n\")\n",
    "    f.write(f\"Target Percentage: {target_pct}%\\n\\n\")\n",
    "    \n",
    "    f.write(\"Test Results Summary:\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(results_df.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Best Model: {best_model_name}\\n\")\n",
    "    f.write(f\"Best F1 Score: {best_f1:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Classification Report (Best Model):\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(classification_report(y_test, y_pred_best, target_names=['No Increase', 'Increase']))\n",
    "\n",
    "print(f\"✓ Test report saved to: {report_path}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ✅ Listed available training sessions\n",
    "2. ✅ Loaded pretrained models and scaler\n",
    "3. ✅ Loaded and prepared test dataset\n",
    "4. ✅ Evaluated all models on test set\n",
    "5. ✅ Visualized performance metrics\n",
    "6. ✅ Analyzed best model in detail\n",
    "7. ✅ Compared training vs test performance\n",
    "8. ✅ Saved test results\n",
    "\n",
    "### Next Steps:\n",
    "- Review test results and identify best performing model\n",
    "- Compare with training performance to check for overfitting\n",
    "- Use best model for backtesting or live trading\n",
    "- Retrain if test performance is significantly lower than training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
