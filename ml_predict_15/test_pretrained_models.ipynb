{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Pretrained Models\n",
    "\n",
    "This notebook loads pretrained models from a selected folder and tests them against a dataset.\n",
    "\n",
    "## Steps:\n",
    "1. Select a training session folder\n",
    "2. Load models and scaler\n",
    "3. Load test dataset\n",
    "4. Evaluate models\n",
    "5. Display results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.data_preparation import prepare_data\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List Available Training Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 training session(s):\n",
      "\n",
      "1. 2025_10_24_3_percent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all training sessions\n",
    "models_base = 'models'\n",
    "sessions = sorted([d for d in os.listdir(models_base) \n",
    "                   if os.path.isdir(os.path.join(models_base, d))],\n",
    "                  reverse=True)\n",
    "\n",
    "print(f\"Found {len(sessions)} training session(s):\\n\")\n",
    "for i, session in enumerate(sessions[:10], 1):  # Show last 10 sessions\n",
    "    session_path = os.path.join(models_base, session)\n",
    "    \n",
    "    # Try to read config file\n",
    "    config_path = os.path.join(session_path, 'training_config.txt')\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            # Extract best model info\n",
    "            best_model = \"N/A\"\n",
    "            best_acc = \"N/A\"\n",
    "            for line in lines:\n",
    "                if \"Best Model:\" in line:\n",
    "                    best_model = line.split(\":\")[1].strip()\n",
    "                if \"Best Accuracy:\" in line:\n",
    "                    best_acc = line.split(\":\")[1].strip()\n",
    "        print(f\"{i}. {session}\")\n",
    "        print(f\"   Best Model: {best_model}, Accuracy: {best_acc}\\n\")\n",
    "    else:\n",
    "        print(f\"{i}. {session}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Training Session\n",
    "\n",
    "Choose which training session to load (default: latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected session: 2025_10_24_3_percent\n",
      "Session path: models\\2025_10_24_3_percent\n"
     ]
    }
   ],
   "source": [
    "# Select session (change this to select a different session)\n",
    "# Option 1: Use latest session\n",
    "selected_session = sessions[0]\n",
    "\n",
    "# Option 2: Manually specify session\n",
    "# selected_session = '2024-01-15_14-30-45'\n",
    "\n",
    "session_path = os.path.join(models_base, selected_session)\n",
    "\n",
    "print(f\"Selected session: {selected_session}\")\n",
    "print(f\"Session path: {session_path}\")\n",
    "\n",
    "# Display training configuration\n",
    "config_path = os.path.join(session_path, 'training_config.txt')\n",
    "if os.path.exists(config_path):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING CONFIGURATION\")\n",
    "    print(\"=\"*80)\n",
    "    with open(config_path, 'r') as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Models and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Scaler loaded from: models\\2025_10_24_3_percent\\scaler.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\tools\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded: decision_tree\n",
      "✓ Loaded: knn\n",
      "✓ Loaded: logistic_regression\n",
      "✓ Loaded: naive_bayes\n",
      "✓ Loaded: ridge_classifier\n",
      "\n",
      "Total models loaded: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\tools\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\tools\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator GaussianNB from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\tools\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelBinarizer from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\tools\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RidgeClassifier from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load scaler\n",
    "scaler_path = os.path.join(session_path, 'scaler.joblib')\n",
    "scaler = joblib.load(scaler_path)\n",
    "print(f\"✓ Scaler loaded from: {scaler_path}\")\n",
    "\n",
    "# Load all models\n",
    "models = {}\n",
    "for file in os.listdir(session_path):\n",
    "    if file.endswith('.joblib') and file != 'scaler.joblib' and not file.endswith('_best.joblib'):\n",
    "        model_name = file.replace('.joblib', '')\n",
    "        model_path = os.path.join(session_path, file)\n",
    "        models[model_name] = joblib.load(model_path)\n",
    "        print(f\"✓ Loaded: {model_name}\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Test Dataset\n",
    "\n",
    "Specify your test dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from: data/btc_2023.csv\n",
      "✓ Test data loaded\n",
      "  Shape: (420657, 5)\n",
      "  Date range: 2023-01-01 00:00:00 to 2023-06-01 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>16530.0</td>\n",
       "      <td>16532.0</td>\n",
       "      <td>16530.0</td>\n",
       "      <td>16532.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>16530.0</td>\n",
       "      <td>16532.0</td>\n",
       "      <td>16530.0</td>\n",
       "      <td>16532.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 00:01:00</td>\n",
       "      <td>16532.0</td>\n",
       "      <td>16532.0</td>\n",
       "      <td>16529.0</td>\n",
       "      <td>16529.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 00:02:00</td>\n",
       "      <td>16528.0</td>\n",
       "      <td>16528.0</td>\n",
       "      <td>16528.0</td>\n",
       "      <td>16528.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 00:03:00</td>\n",
       "      <td>16528.0</td>\n",
       "      <td>16528.0</td>\n",
       "      <td>16525.0</td>\n",
       "      <td>16525.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Timestamp     Open     High      Low    Close\n",
       "0  2023-01-01 00:00:00  16530.0  16532.0  16530.0  16532.0\n",
       "1  2023-01-01 00:00:00  16530.0  16532.0  16530.0  16532.0\n",
       "2  2023-01-01 00:01:00  16532.0  16532.0  16529.0  16529.0\n",
       "3  2023-01-01 00:02:00  16528.0  16528.0  16528.0  16528.0\n",
       "4  2023-01-01 00:03:00  16528.0  16528.0  16525.0  16525.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "# Change this to your test dataset path\n",
    "test_data_path = 'data/btc_2023.csv'\n",
    "\n",
    "print(f\"Loading test data from: {test_data_path}\")\n",
    "df_test = pd.read_csv(test_data_path)\n",
    "\n",
    "print(f\"✓ Test data loaded\")\n",
    "print(f\"  Shape: {df_test.shape}\")\n",
    "print(f\"  Date range: {df_test['Timestamp'].min()} to {df_test['Timestamp'].max()}\")\n",
    "\n",
    "# Display first few rows\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test data with target_bars=45, target_pct=3.0%\n",
      "✓ Test data prepared\n",
      "  Features shape: (420612, 5)\n",
      "  Target shape: (420612,)\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "0    419463\n",
      "1      1149\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class balance:\n",
      "target\n",
      "0    0.997268\n",
      "1    0.002732\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "# Use same parameters as training (adjust if needed)\n",
    "target_bars = 45\n",
    "target_pct = 3.0\n",
    "\n",
    "print(f\"Preparing test data with target_bars={target_bars}, target_pct={target_pct}%\")\n",
    "X_test, y_test = prepare_data(df_test, target_bars=target_bars, target_pct=target_pct)\n",
    "\n",
    "print(f\"✓ Test data prepared\")\n",
    "print(f\"  Features shape: {X_test.shape}\")\n",
    "print(f\"  Target shape: {y_test.shape}\")\n",
    "\n",
    "# Display class distribution\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y_test.value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scale Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- BOLLINGER_cross_mid\n- RSI_cross_min\n- SMA_cross\n- SMA_cross_10\n- STOCH_cross_min\nFeature names seen at fit time, yet now missing:\n- SMA_10\n- SMA_10_15\n- SMA_15\n- SMA_5\n- SMA_5_10\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Scale features using loaded scaler\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Test data scaled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Scaled features shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test_scaled\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\tools\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mC:\\tools\\Anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:534\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    530\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    532\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 534\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    535\u001b[0m     X,\n\u001b[0;32m    536\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[0;32m    537\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[0;32m    538\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    539\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    540\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    541\u001b[0m )\n\u001b[0;32m    543\u001b[0m X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    544\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n",
      "File \u001b[1;32mC:\\tools\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mC:\\tools\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- BOLLINGER_cross_mid\n- RSI_cross_min\n- SMA_cross\n- SMA_cross_10\n- STOCH_cross_min\nFeature names seen at fit time, yet now missing:\n- SMA_10\n- SMA_10_15\n- SMA_15\n- SMA_5\n- SMA_5_10\n"
     ]
    }
   ],
   "source": [
    "# Scale features using loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"✓ Test data scaled\")\n",
    "print(f\"  Scaled features shape: {X_test_scaled.shape}\")\n",
    "print(f\"  Min value: {X_test_scaled.min():.4f}\")\n",
    "print(f\"  Max value: {X_test_scaled.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATING MODELS ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating: {model_name}...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Get prediction probabilities if available\n",
    "    try:\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    except:\n",
    "        roc_auc = np.nan\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1_Score': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'ROC_AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    if not np.isnan(roc_auc):\n",
    "        print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1_Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_f1 = results_df.iloc[0]['F1_Score']\n",
    "print(f\"\\n✓ Best Model: {best_model_name} (F1 Score: {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Model Performance on Test Set', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['Accuracy', 'F1_Score', 'Precision', 'Recall', 'ROC_AUC']\n",
    "titles = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC AUC']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Sort by metric\n",
    "    sorted_df = results_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.barh(sorted_df['Model'], sorted_df[metric])\n",
    "    \n",
    "    # Color best model differently\n",
    "    best_idx = sorted_df[metric].idxmax()\n",
    "    bars[list(sorted_df.index).index(best_idx)].set_color('green')\n",
    "    \n",
    "    ax.set_xlabel(title, fontsize=12)\n",
    "    ax.set_title(f'{title} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (idx, row) in enumerate(sorted_df.iterrows()):\n",
    "        value = row[metric]\n",
    "        if not np.isnan(value):\n",
    "            ax.text(value, i, f' {value:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detailed Analysis of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"DETAILED ANALYSIS: {best_model_name.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['No Increase', 'Increase']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Increase', 'Increase'],\n",
    "            yticklabels=['No Increase', 'Increase'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives:  {tn:,}\")\n",
    "print(f\"  False Positives: {fp:,}\")\n",
    "print(f\"  False Negatives: {fn:,}\")\n",
    "print(f\"  True Positives:  {tp:,}\")\n",
    "\n",
    "print(f\"\\nAdditional Metrics:\")\n",
    "print(f\"  Specificity: {tn / (tn + fp):.4f}\")\n",
    "print(f\"  Sensitivity (Recall): {tp / (tp + fn):.4f}\")\n",
    "print(f\"  False Positive Rate: {fp / (fp + tn):.4f}\")\n",
    "print(f\"  False Negative Rate: {fn / (fn + tp):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare Training vs Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training results\n",
    "training_csv_path = os.path.join(session_path, 'training_results_summary.csv')\n",
    "\n",
    "if os.path.exists(training_csv_path):\n",
    "    training_results = pd.read_csv(training_csv_path)\n",
    "    \n",
    "    # Filter out summary rows\n",
    "    training_results = training_results[~training_results['Model'].str.contains('SUMMARY|Best Model|Total|Average', na=False)]\n",
    "    \n",
    "    # Merge with test results\n",
    "    comparison = training_results[['Model', 'Accuracy', 'F1_Score']].merge(\n",
    "        results_df[['Model', 'Accuracy', 'F1_Score']],\n",
    "        on='Model',\n",
    "        suffixes=('_Train', '_Test')\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TRAINING vs TEST PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(comparison.to_string(index=False))\n",
    "    \n",
    "    # Calculate performance drop\n",
    "    comparison['Accuracy_Drop'] = comparison['Accuracy_Train'] - comparison['Accuracy_Test']\n",
    "    comparison['F1_Drop'] = comparison['F1_Score_Train'] - comparison['F1_Score_Test']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERFORMANCE DROP (Training - Test)\")\n",
    "    print(\"=\"*80)\n",
    "    print(comparison[['Model', 'Accuracy_Drop', 'F1_Drop']].to_string(index=False))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    x = np.arange(len(comparison))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0].bar(x - width/2, comparison['Accuracy_Train'], width, label='Training', alpha=0.8)\n",
    "    axes[0].bar(x + width/2, comparison['Accuracy_Test'], width, label='Test', alpha=0.8)\n",
    "    axes[0].set_xlabel('Model', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Accuracy: Training vs Test', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(comparison['Model'], rotation=45, ha='right')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # F1 Score comparison\n",
    "    axes[1].bar(x - width/2, comparison['F1_Score_Train'], width, label='Training', alpha=0.8)\n",
    "    axes[1].bar(x + width/2, comparison['F1_Score_Test'], width, label='Test', alpha=0.8)\n",
    "    axes[1].set_xlabel('Model', fontsize=12)\n",
    "    axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "    axes[1].set_title('F1 Score: Training vs Test', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(comparison['Model'], rotation=45, ha='right')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Comparison complete\")\n",
    "else:\n",
    "    print(\"Training results CSV not found. Skipping comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test results to session directory\n",
    "test_results_path = os.path.join(session_path, 'test_results.csv')\n",
    "results_df.to_csv(test_results_path, index=False)\n",
    "\n",
    "print(f\"✓ Test results saved to: {test_results_path}\")\n",
    "\n",
    "# Save detailed report\n",
    "report_path = os.path.join(session_path, 'test_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"TEST RESULTS REPORT\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(f\"Training Session: {selected_session}\\n\")\n",
    "    f.write(f\"Test Dataset: {test_data_path}\\n\")\n",
    "    f.write(f\"Test Set Size: {len(y_test)}\\n\")\n",
    "    f.write(f\"Target Bars: {target_bars}\\n\")\n",
    "    f.write(f\"Target Percentage: {target_pct}%\\n\\n\")\n",
    "    \n",
    "    f.write(\"Test Results Summary:\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(results_df.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Best Model: {best_model_name}\\n\")\n",
    "    f.write(f\"Best F1 Score: {best_f1:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Classification Report (Best Model):\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(classification_report(y_test, y_pred_best, target_names=['No Increase', 'Increase']))\n",
    "\n",
    "print(f\"✓ Test report saved to: {report_path}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ✅ Listed available training sessions\n",
    "2. ✅ Loaded pretrained models and scaler\n",
    "3. ✅ Loaded and prepared test dataset\n",
    "4. ✅ Evaluated all models on test set\n",
    "5. ✅ Visualized performance metrics\n",
    "6. ✅ Analyzed best model in detail\n",
    "7. ✅ Compared training vs test performance\n",
    "8. ✅ Saved test results\n",
    "\n",
    "### Next Steps:\n",
    "- Review test results and identify best performing model\n",
    "- Compare with training performance to check for overfitting\n",
    "- Use best model for backtesting or live trading\n",
    "- Retrain if test performance is significantly lower than training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
